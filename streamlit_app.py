import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import plotly.colors as pc
import os
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
import seaborn as sns

################## CARREGAR FICHEIROS ################################
base = os.path.dirname(os.path.abspath(__file__))

# Caminho completo para o ficheiro
file = os.path.join(base, "dados_streamlit", "termos_titulos.csv")

# Verificar se o ficheiro existe antes de abrir
if os.path.exists(file):
    # Ler o ficheiro CSV com pandas
    termos_titulos = pd.read_csv(file)
    print("Ficheiro carregado com sucesso!")
else:
    print(f"Ficheiro n√£o encontrado: {file}")

base = os.path.dirname(os.path.abspath(__file__))

# Caminho completo para o ficheiro
file = os.path.join(base, "dados_streamlit", "df_combined.csv")

# Verificar se o ficheiro existe antes de abrir
if os.path.exists(file):
    # Ler o ficheiro CSV com pandas
    df_all = pd.read_csv(file)
    print("Ficheiro carregado com sucesso!")
else:
    print(f"Ficheiro n√£o encontrado: {file}")

base = os.path.dirname(os.path.abspath(__file__))

# Caminho completo para o ficheiro
file = os.path.join(base, "dados_streamlit", "df_authors.csv")

# Verificar se o ficheiro existe antes de abrir
if os.path.exists(file):
    # Ler o ficheiro CSV com pandas
    df_authors = pd.read_csv(file)
    print("Ficheiro carregado com sucesso!")
else:
    print(f"Ficheiro n√£o encontrado: {file}")

file = os.path.join(base, "dados_streamlit", "df_combined.csv")
# Verificar se o ficheiro existe antes de abrir
if os.path.exists(file):
    # Ler o ficheiro CSV com pandas
    df_combined = pd.read_csv(file)
    print("Ficheiro carregado com sucesso!")
else:
    print(f"Ficheiro n√£o encontrado: {file}")

print(df_combined.head())


top_10_techniques_by_cluster = pd.read_csv(r'/workspaces/Dashboard_ICD/dados_streamlit/top_10_techniques_by_cluster.csv')

################### PREPARA√á√ÉO DOS GR√ÅFICOS ##########################

# URL do VOSviewer
url = "https://tinyurl.com/233s5fwm"

# Definir as cores customizadas
custom_colors = [
    "#003f5b", "#2b4b7d", "#5f5195", "#98509d", 
    "#cc4c91", "#f25375", "#ff6f4e", "#ff9913"]

# Criar o gr√°fico de barras interativo para visualizar os 10 primeiros termos
fig = px.bar(
    termos_titulos.head(10),
    x='Count',
    y='Term',
    orientation='h',
    labels={'Count': 'Frequ√™ncia', 'Term': 'Termo'},
    color='Count',
    color_continuous_scale=custom_colors
)

# Ajustar o layout do gr√°fico
fig.update_layout(
    xaxis_title='Frequ√™ncia',
    yaxis_title='Termos',
    yaxis=dict(autorange='reversed'),  # Inverter a ordem dos termos no eixo y
    height=500,  # Aumentar a altura do gr√°fico
    margin=dict(l=30, r=30, t=40, b=40)  # Ajustar margens para dar mais espa√ßo
)

# Layout do Streamlit
st.set_page_config(
    page_title="Dashboard - Data Science e Big Data no Planeamento Urbano",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

df_all['count'] = 1  # Inicializar com 1 para representar cada artigo
df_all = df_all.groupby(['ano', 'affiliation-country'], as_index=False).agg({'count': 'sum'})

# Encontrar o valor m√°ximo de 'count' em qualquer pa√≠s ao longo de todos os anos
max_count = df_all['count'].max()

# Criar o mapa interativo
fig1 = px.choropleth(
    df_all,
    locations="affiliation-country",  # Nome do pa√≠s
    locationmode="country names",  # Usando nomes de pa√≠ses
    color="count",  # Usando a coluna 'count' para a cor
    color_continuous_scale=custom_colors,
    hover_name="affiliation-country",  # Para exibir o nome do pa√≠s
    animation_frame="ano",  # Para animar o mapa ao longo dos anos
    range_color=[0, max_count]  # Define a escala de cor de 0 at√© o valor m√°ximo fixo
)

# Ajustar o layout do mapa com fundo preto
fig1.update_layout(
    geo=dict(
        showframe=False, 
        showcoastlines=True, 
        projection_type='natural earth',
        bgcolor="black"  # Define o fundo do mapa como preto
    ),
    coloraxis_colorbar=dict(title="N√∫mero de Artigos"),
    paper_bgcolor="black",  # Fundo da √°rea de papel tamb√©m preto
    plot_bgcolor="black"    # Fundo do gr√°fico tamb√©m preto
)


# Definir as colunas obrigat√≥rias
# Defini√ß√£o das colunas e dados (ajustar conforme necess√°rio)
required_columns = ['author', 'n_artigos_pub', 'affiliation', 'country']
if not all(column in df_authors.columns for column in required_columns):
    raise ValueError(f"O DataFrame deve conter as colunas: {required_columns}")

# Criar uma coluna com as siglas das institui√ß√µes
df_authors['institution_labels_simp'] = df_authors['affiliation'].apply(lambda x: ''.join([word[0].upper() for word in x.split()]))

# Selecionar os 10 autores com mais publica√ß√µes
top_authors_df = df_authors.nlargest(10, 'n_artigos_pub')

# Preparar os dados para o diagrama de Sankey
authors = top_authors_df['author'].tolist()
affiliations = top_authors_df['affiliation'].tolist()
countries = top_authors_df['country'].dropna().unique().tolist()
institution_labels_simp = [df_authors[df_authors['affiliation'] == inst]['institution_labels_simp'].values[0] for inst in affiliations]

# Criar labels para pa√≠ses, institui√ß√µes (simplificadas) e top 10 autores
labels = countries + institution_labels_simp + authors

# Criar dicion√°rios para mapear os √≠ndices
country_indices = {country: i for i, country in enumerate(countries)}
affiliation_indices = {affiliation: i + len(countries) for i, affiliation in enumerate(institution_labels_simp)}
author_indices = {author: i + len(countries) + len(institution_labels_simp) for i, author in enumerate(authors)}

# Criar listas para as fontes e destinos
sources = []
targets = []
values = []
link_colors = []

# Definir cores para os pa√≠ses
custom_colors = [
    "#5f5195", "#98509d", 
    "#cc4c91", "#f25375",
    "#ff6f4e", "#ff9913"
]
country_colors = custom_colors[:len(countries)]

# Adicionar rela√ß√µes pa√≠s -> institui√ß√£o
for i, row in top_authors_df.iterrows():
    sources.append(country_indices[row['country']])
    targets.append(affiliation_indices[institution_labels_simp[affiliations.index(row['affiliation'])]])
    values.append(row['n_artigos_pub'])
    link_colors.append(country_colors[countries.index(row['country'])])

# Adicionar rela√ß√µes institui√ß√£o -> autor
for i, row in top_authors_df.iterrows():
    sources.append(affiliation_indices[institution_labels_simp[affiliations.index(row['affiliation'])]])
    targets.append(author_indices[row['author']])
    values.append(row['n_artigos_pub'])
    # Criar uma cor mais clara para o autor
    base_color = pc.hex_to_rgb(country_colors[countries.index(row['country'])])
    lighter_color = pc.find_intermediate_color(base_color, (255, 255, 255), 0.5)
    lighter_color = f"rgb{lighter_color}"
    link_colors.append(lighter_color)

# Criar customdata para incluir o nome completo e o n√∫mero de artigos
customdata = []
for label in labels:
    if label in authors:
        count = top_authors_df[top_authors_df['author'] == label]['n_artigos_pub'].values[0]
        customdata.append(f"{label}<br>Number of articles: {count}")
    elif label in institution_labels_simp:
        full_name = affiliations[institution_labels_simp.index(label)]
        count = top_authors_df[top_authors_df['affiliation'] == full_name]['n_artigos_pub'].sum()
        customdata.append(f"{full_name}<br>Number of articles: {count}")
    else:
        count = top_authors_df[top_authors_df['country'] == label]['n_artigos_pub'].sum()
        customdata.append(f"{label}<br>Number of articles: {count}")

# Criar o diagrama de Sankey
fig2 = go.Figure(data=[go.Sankey(
    node=dict(
        pad=20,
        thickness=20,  # Espessura dos n√≥s ajustada
        line=dict(color="white", width=0),  # Remover o contorno dos n√≥s (sem outline)
        label=labels,
        customdata=customdata,  # Texto completo para o pop-up
        hovertemplate='%{customdata}<extra></extra>',  # Formato do pop-up
        color="#003f5c",  # Cor dos n√≥s ajustada para cinza
    ),
    link=dict(
        source=sources,
        target=targets,
        value=values,
        color=link_colors
    )
)])

# Ajuste do layout para tornar o gr√°fico mais clean e bonito
fig2.update_layout(
    hovermode='x',
    font=dict(size=14, color='Black', family="Arial"),  # Aumentar o tamanho da fonte
    plot_bgcolor='black',  # Fundo preto
    paper_bgcolor='black',  # Fundo da p√°gina tamb√©m preto
    width=700,  # Largura do gr√°fico mais ajustada
    height=500,  # Altura ajustada para um aspecto mais equilibrado
    showlegend=False  # Remover a legenda para n√£o sobrecarregar o gr√°fico
)

# wordcloud

#wordcloud
custom_colors = [
    "#5f5195", "#98509d", 
    "#cc4c91", "#f25375",
    "#ff6f4e", "#ff9913"
]
df_combined['processedAbstract'] = df_combined['processedAbstract'].str.replace(r"[']", "", regex=True)
df_combined['processedAbstract'] = df_combined['processedAbstract'].str.replace(r"[,]", "", regex=True)
df_combined['processedAbstract'] = df_combined['processedAbstract'].str.replace(r"[.]", "", regex=True)

# Fun√ß√£o que mapeia a cor das palavras
def color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    return custom_colors[hash(word) % len(custom_colors)]

# Gerar a wordcloud
plt.figure(figsize=(16,13))

wordcloud = WordCloud(
    background_color='black',
    max_words=10000,
    width=800,
    height=400,
    color_func=color_func  # Definir a fun√ß√£o de cores personalizada
).generate(" ".join(df_combined['processedAbstract']))

# Criar o gr√°fico no matplotlib
fig5, ax = plt.subplots(figsize=(10, 6), facecolor='black')  # Define a moldura preta
ax.imshow(wordcloud, interpolation='bilinear')
ax.axis('off')  # Remove os eixos
ax.set_facecolor('black')  # Define o fundo do gr√°fico como preto

num_topics = 4

# Contagem de artigos por pa√≠s
country_counts = df_combined['affiliation-country'].value_counts()

# Selecionando os 10 pa√≠ses com mais artigos
top_10_countries = country_counts.head(10).index

# Criando um DataFrame para contar os t√≥picos por pa√≠s
topic_counts = pd.DataFrame(columns=[f'T√≥pico {i}' for i in range(num_topics)], index=top_10_countries)

# Inicializando as contagens dos t√≥picos com 0
for country in top_10_countries:
    topic_counts.loc[country] = np.zeros(num_topics)

# Contagem dos t√≥picos por pa√≠s
for i, row in df_combined.iterrows():
    country = row['affiliation-country']
    if country in top_10_countries:
        topic_dist = row['topic_distribution']  # Supondo que 'topic_distribution' pode ser uma string ou uma lista de tuplas
        
        # Verificando se 'topic_dist' √© uma string (caso seja uma representa√ß√£o de lista)
        if isinstance(topic_dist, str):
            try:
                topic_dist = eval(topic_dist)  # Converte a string para uma lista de tuplas
            except Exception as e:
                print(f"Erro ao tentar converter 'topic_distribution' para lista em {country}. Valor: {topic_dist}, Erro: {e}")
                continue
        
        # Verificando se 'topic_dist' √© uma lista de tuplas
        if isinstance(topic_dist, list) and all(isinstance(item, tuple) and len(item) == 2 for item in topic_dist):
            for topic, prob in topic_dist:
                # Asegurando que 'topic' √© um √≠ndice v√°lido e 'prob' √© num√©rico
                if isinstance(topic, int) and 0 <= topic < num_topics and isinstance(prob, (int, float)):
                    topic_counts.loc[country, f'T√≥pico {topic}'] += prob
                else:
                    print(f"Distribui√ß√£o de t√≥pico inv√°lida encontrada em {country}. T√≥pico: {topic}, Probabilidade: {prob}")
        else:
            print(f"'topic_distribution' n√£o √© uma lista de tuplas v√°lida em {country}. Valor encontrado: {topic_dist}")

# Garantindo que todos os valores de 'topic_counts' s√£o num√©ricos e substituindo NaNs por 0
topic_counts = topic_counts.apply(pd.to_numeric, errors='coerce').fillna(0)

# Normalizando os valores para percentagem
topic_counts_percentage = topic_counts.div(topic_counts.sum(axis=1), axis=0) * 100

# Contagem dos t√≥picos por ano
topic_counts_by_year = pd.DataFrame(0, index=df_combined['ano'].unique(), columns=[f'Topic {i}' for i in range(num_topics)])

# Preenchendo a tabela com a distribui√ß√£o dos t√≥picos por ano
for idx, row in df_combined.iterrows():
    year = row['ano']
    topic_dist = row['topic_distribution']  # Distribui√ß√£o de t√≥picos (lista de probabilidades)
    if isinstance(topic_dist, str):
        try:
            topic_dist = eval(topic_dist)  # Converte a string para uma lista de tuplas
        except Exception as e:
            print(f"Erro ao tentar converter 'topic_distribution' para lista no ano {year}. Valor: {topic_dist}, Erro: {e}")
            continue
    # Verificando se 'topic_dist' √© uma lista de tuplas
    if isinstance(topic_dist, list) and all(isinstance(item, tuple) and len(item) == 2 for item in topic_dist):
        for topic, prob in topic_dist:
            if isinstance(topic, int) and 0 <= topic < num_topics and isinstance(prob, (int, float)):
                topic_counts_by_year.loc[year, f'Topic {topic}'] += prob
            else:
                print(f"Distribui√ß√£o de t√≥pico inv√°lida encontrada no ano {year}. T√≥pico: {topic}, Probabilidade: {prob}")

# Normalizando os valores para percentagem
topic_counts_by_year_percentage = topic_counts_by_year.div(topic_counts_by_year.sum(axis=1), axis=0) * 100

topic_counts_by_year_percentage = topic_counts_by_year_percentage.T
topic_counts_by_year_percentage = topic_counts_by_year_percentage.sort_index(axis=1)


################# styling ##########################################
# CSS styling para o fundo da rede
st.markdown("""
<div style="background-color: black; padding: 20px; border-radius: 5px; text-align: left;">
    <h1 style="font-size: 40px; font-weight: bold; color: #bc5090;">AN√ÅLISE DE ARTIGOS CIENT√çFICOS</h1>
    <h2 style="font-size: 32px; font-weight: normal; color: #ffa600;">DATA SCIENCE e BIG DATA, aplicadas ao PLANEAMENTO URBANO</h2>
</div>

<style>
/* Estilo do iframe da rede com fundo preto */
iframe {
    background-color: black;
}
</style>
""", unsafe_allow_html=True)


tab1, tab2 = st.tabs([" üìà An√°lise Bibliom√©trica", " üìä An√°lise de Conte√∫do"])

# Aba 1: An√°lise Bibliogr√°fica
with tab1:
    st.markdown("""
<div style="background-color: #333; padding: 10px; border-radius: 5px; color: white; font-size: 16px; margin-bottom: 20px;">
    Query da pesquisa SCOPUS: Data Science OR Big Data AND (Urban Planning OR Urban Management OR Spatial Planning OR Urban Development), limited to Social Sciences
</div>
""", unsafe_allow_html=True)

    col1, spacer1, col2 = st.columns([3, 0.2, 3])
    # Dividir a tela em tr√™s colunas, conforme exemplo fornecido
    with col1:
        with st.container():
            st.subheader('Rede bibliom√©trica de co-ocorr√™ncia de keywords dos autores')
            st.components.v1.html(f'<iframe src="{url}" width="95%" height="500px" style="border: 0px solid #ddd; max-width: 1000px; max-height: 500px; background-color: black;"></iframe>', height=600)
    
    with spacer1:
        st.markdown(' ')
    
    with col2:
        with st.container():
            st.subheader('Pa√≠s de publica√ß√£o dos artigos')
            st.plotly_chart(fig1, use_container_width=True,
                        width=500, height=500,
                        key="fig1")

    st.markdown(" ")
    st.markdown(" ")
    st.markdown(" ")

    col3, spacer2, col4 = st.columns([3, 0.2, 3])

    with col3:
        st.subheader('Top 10: Termos mais recorrentes nos t√≠tulos dos artigos')
        st.plotly_chart(fig, use_container_width=True,
                        width=500, height=500,
                        key="fig")

    with spacer2:
        st.markdown(' ')

    with col4:
        st.subheader('TOP 10: Autores, por n¬∫ de publica√ß√µes')
        st.plotly_chart(fig2,
                        width=500, height=500, key="fig2"
                        )
with tab2:

    # Dividir a tela em tr√™s colunas com espa√ßadores
    col1, spacer1, col2 = st.columns([2, 6, 2])
    with col1:
        st.markdown(' ')
    with spacer1:
        with st.container():
            st.subheader('Palavras mais comuns no abstract dos artigos')
            plt.figure(figsize=(10, 6))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')  # Remove os eixos
            st.pyplot(plt)
    with col2:
        st.markdown(' ')

    st.markdown(" ")
    st.markdown(" ")
    st.markdown(" ")

# Dividir a tela em tr√™s colunas com espa√ßadores
    col1, spacer1, col2 = st.columns([1, 6, 1])
    with col1:
        st.markdown(' ')
    with spacer1:
        with st.container():
            st.markdown("""
    <div style="background-color: #333; padding: 10px; border-radius: 5px; color: white; font-size: 24px; margin-bottom: 20px;">
        T√≥picos gerados, utilizando o modelo LDA:
    </div>
    """, unsafe_allow_html=True)
            st.markdown("""
            **T√≥pico 0:**
            - **Categoria sugerida**: *Data Science and Urban Analysis*
            - Baseado nos termos como "data", "big", "analysis", "systems", "media", e "knowledge", este cluster parece se concentrar no uso de ci√™ncia de dados, m√©todos computacionais e an√°lise de grandes volumes de informa√ß√£o aplicados a contextos urbanos.

            **T√≥pico 1:**
            - **Categoria sugerida**: *Spatial and Population Dynamics*
            - Os termos como "spatial", "population", "distribution", "network", "interaction", e "structure" indicam um foco em estudos de din√¢mica espacial, popula√ß√£o e estrutura urbana, incluindo redes e intera√ß√µes entre √°reas urbanas e industriais.

            **T√≥pico 2:**
            - **Categoria sugerida**: *Smart and Sustainable Cities*
            - Termos como "smart", "cities", "sustainable", "digital", "technologies", e "governance" sugerem que este cluster trata de cidades inteligentes, desenvolvimento sustent√°vel, uso de tecnologias digitais e governan√ßa urbana moderna.

            **T√≥pico 3:**
            - **Categoria sugerida**: *Urban Land Use and Activity Patterns*
            - Com termos como "land", "vitality", "spatial", "travel", "activity", e "patterns", este cluster parece abordar padr√µes de uso do solo, mobilidade, e a vitalidade ou vibr√¢ncia das √°reas urbanas.
            """)
    with col2:
        st.markdown(' ')

    st.markdown(" ")
    st.markdown(" ")
    st.markdown(" ")


    # Segunda linha de colunas
    col3, spacer2, col4 = st.columns([3, 0.2, 3])

    with col3:
        st.subheader('Distribui√ß√£o de T√≥picos por Pa√≠s (%)')
        plt.style.use('dark_background')
            # Gerando o gr√°fico do heatmap
        plt.figure(figsize=(12, 8))
        sns.heatmap(topic_counts_percentage, annot=True, cmap='YlGnBu', fmt=".2f", linewidths=0.5)

        # Alterando t√≠tulo e labels para garantir boa visibilidade em fundo escuro
        plt.xlabel("T√≥picos", fontsize=12, color='white')
        plt.ylabel("Pa√≠ses", fontsize=12, color='white')

        # Exibindo o gr√°fico no Streamlit
        st.pyplot(plt)

    with spacer2:
        st.markdown(' ')

    with col4:
        st.subheader('Distribui√ß√£o de T√≥picos por Ano de Publica√ß√£o (%)')
        # Visualizando o heatmap com os valores em percentagem
        plt.style.use('dark_background')
        plt.figure(figsize=(14, 8))
        sns.heatmap(topic_counts_by_year_percentage, annot=True, cmap='YlGnBu', fmt=".2f", linewidths=0.5)
        plt.xlabel("T√≥picos")
        plt.ylabel("Ano de Publica√ß√£o")

         # Alterando t√≠tulo e labels para garantir boa visibilidade em fundo escuro
        plt.xlabel("Ano de publica√ß√£o", fontsize=12, color='white')
        plt.ylabel("T√≥picos", fontsize=12, color='white')

        st.pyplot(plt)

    col1, spacer1, col2 = st.columns([1, 6, 1])
    with col1:
        st.markdown(' ')
    with spacer1:
        st.subheader('T√©cnicas de Data Science mais comuns por t√≥pico:')
        top_10_techniques_by_cluster.rename(columns={
        'Topic_Cluster': 'T√≥pico', 
        'tecnicas_str_categorizadas': 'T√©cnicas',
        'percentage': '%'
        # Adicione outras colunas que voc√™ deseja renomear aqui
 }, inplace=True)
        cluster_choice = st.selectbox("Escolha um Topic Cluster para ver as t√©cnicas mais comuns", top_10_techniques_by_cluster['T√≥pico'].unique())
        filtered_df = top_10_techniques_by_cluster[top_10_techniques_by_cluster['T√≥pico'] == cluster_choice]

        st.write(f"T√©cnicas mais comuns no t√≥pico {cluster_choice}:")
        st.dataframe(filtered_df)
    with col2:
        st.markdown(' ')
        st.dataframe(filtered_df)
    with col2:
        st.markdown(' ')
